{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMF3oHaR4q6/l+0WkFqqwfV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keerthana8888/Traffic-Analysis_ComputerVision/blob/main/Code%20File/Traffic_Analysis_ComputerVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Approach To Traffic Analysis"
      ],
      "metadata": {
        "id": "IooW7OLptDOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install and Update Libraries\n",
        "2. Project Objectives\n",
        "  * Model Configurations\n",
        "  * Reading Video with CV2\n",
        "  * Capturing Video Information\n",
        "  * Video Output\n",
        "  * Executing Recognition\n",
        "  * Post Processing\n",
        "\n",
        "3. Sampling Transformed Frame Results\n",
        "4. Executing Result Video\n",
        "5. Conclusion"
      ],
      "metadata": {
        "id": "sxNsM6HetKGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading The Libraries"
      ],
      "metadata": {
        "id": "zYjgaBbxuBRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDINJriq_OBN"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Object Detecion\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#plots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Display image and videos\n",
        "import IPython\n",
        "from IPython.display import Video, display\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "xRZmSgl9_S-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project Objective"
      ],
      "metadata": {
        "id": "mZMfIjNF5h0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Detect vehicles in an highway - (cars, trucks, bus and motorcycles)\n",
        " - Count the number of vehicles that pass thought a transition line indicating the direction they are going to. ( in\\ou)\n",
        " - Count the total number of each class have passed in the highway\n"
      ],
      "metadata": {
        "id": "PEX9Hqcq5q52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Video path for experiment\n",
        "path = '/content/british_highway_traffic.mp4'"
      ],
      "metadata": {
        "id": "8Fz10LaF_xQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the target video\n",
        "frac = 0.65\n",
        "display(Video(data=path, height=int(720*frac), width=int(1280*frac)))"
      ],
      "metadata": {
        "id": "U0k6UXU4AKmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading a YOLO model\n",
        "model = YOLO('yolov8x.pt')\n",
        "\n",
        "#geting names from classes\n",
        "dict_classes = model.model.names"
      ],
      "metadata": {
        "id": "_sYLeKvyAbX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.info(detailed=True)"
      ],
      "metadata": {
        "id": "zsOImKocHTyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_classes"
      ],
      "metadata": {
        "id": "dVrc-OpfHbPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configurations"
      ],
      "metadata": {
        "id": "VcvIc7vh_6Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_frame(frame, scale_percent):\n",
        "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
        "    if frame is None:\n",
        "        return frame\n",
        "    width = int(frame.shape[1] * scale_percent / 100)\n",
        "    height = int(frame.shape[0] * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    return cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)"
      ],
      "metadata": {
        "id": "qJQ4V3UFAfvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Configurations\n",
        "#Verbose during prediction\n",
        "verbose = False\n",
        "# Scaling percentage of original frame\n",
        "scale_percent = 50"
      ],
      "metadata": {
        "id": "UD1XpYfrByH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Video with cv2"
      ],
      "metadata": {
        "id": "rCEKVSX_AAlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading video with cv2\n",
        "video = cv2.VideoCapture(path)\n",
        "\n",
        "# Objects to detect Yolo\n",
        "class_IDS = [2, 3, 5, 7]\n",
        "# Auxiliary variables\n",
        "centers_old = {}\n",
        "centers_new = {}\n",
        "obj_id = 0\n",
        "veiculos_contador_in = dict.fromkeys(class_IDS, 0)\n",
        "veiculos_contador_out = dict.fromkeys(class_IDS, 0)\n",
        "end = []\n",
        "frames_list = []\n",
        "cy_linha = int(1500 * scale_percent/100 )\n",
        "cx_sentido = int(2000 * scale_percent/100)\n",
        "offset = int(8 * scale_percent/100 )\n",
        "contador_in = 0\n",
        "contador_out = 0\n",
        "print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
        "\n",
        "\n",
        "# Original informations of video\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "print('[INFO] - Original Dim: ', (width, height))\n",
        "\n",
        "# Scaling Video for better performance\n",
        "if scale_percent != 100:\n",
        "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
        "    width = int(width * scale_percent / 100)\n",
        "    height = int(height * scale_percent / 100)\n",
        "    print('[INFO] - Dim Scaled: ', (width, height))"
      ],
      "metadata": {
        "id": "MGx2OQi-CDOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capturing Video Information"
      ],
      "metadata": {
        "id": "Nek6a1KDAISq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original informations of video\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "print('[INFO] - Original Dim: ', (width, height))\n",
        "\n",
        "# Scaling Video for better performance\n",
        "if scale_percent != 100:\n",
        "    print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
        "    width = int(width * scale_percent / 100)\n",
        "    height = int(height * scale_percent / 100)\n",
        "    print('[INFO] - Dim Scaled: ', (width, height))"
      ],
      "metadata": {
        "id": "R_cRbJfACLFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Output"
      ],
      "metadata": {
        "id": "Jvb_s7lxAQtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------\n",
        "### Video output ####\n",
        "video_name = 'result.mp4'\n",
        "output_path = \"rep_\" + video_name\n",
        "tmp_output_path = \"tmp_\" + output_path\n",
        "VIDEO_CODEC = \"MP4V\"\n",
        "\n",
        "output_video = cv2.VideoWriter(tmp_output_path,\n",
        "                               cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n",
        "                               fps, (width, height))\n"
      ],
      "metadata": {
        "id": "G7R_GdaSrquQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing Recognition"
      ],
      "metadata": {
        "id": "mZZAn7QRC32g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------\n",
        "# Executing Recognition\n",
        "for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)) - 1)):\n",
        "\n",
        "    # reading frame from video\n",
        "    _, frame = video.read()\n",
        "\n",
        "    #Applying resizing of read frame\n",
        "\n",
        "    frame  = resize_frame(frame, scale_percent)\n",
        "\n",
        "    if verbose:\n",
        "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
        "\n",
        "    # Getting predictions\n",
        "    y_hat = model.predict(frame, conf = 0.7, classes = class_IDS, device = 0, verbose = False)\n",
        "\n",
        "    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n",
        "    boxes   = y_hat[0].boxes.xyxy.cpu().numpy()\n",
        "    conf    = y_hat[0].boxes.conf.cpu().numpy()\n",
        "    classes = y_hat[0].boxes.cls.cpu().numpy()\n",
        "\n",
        "    # Storing the above information in a dataframe\n",
        "    positions_frame = pd.DataFrame(y_hat[0].cpu().numpy().boxes.data, columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
        "\n",
        "    #Translating the numeric class labels to text\n",
        "    labels = [dict_classes[i] for i in classes] #lables(car,truck)\n",
        "\n",
        "    # Drawing transition line for in\\out vehicles counting\n",
        "    cv2.line(frame, (0, cy_linha), (int(4500 * scale_percent/100 ), cy_linha), (255,255,0),8) #imaginary line\n",
        "\n",
        "    # For each vehicles, draw the bounding-box and counting each one the pass thought the transition line (in\\out)\n",
        "    for ix, row in enumerate(positions_frame.iterrows()):\n",
        "        # Getting the coordinates of each vehicle (row)\n",
        "        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n",
        "\n",
        "        # Calculating the center of the bounding-box\n",
        "        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2) #due to scaling down we divide by 2\n",
        "\n",
        "        # drawing center and bounding-box of vehicle in the given frame\n",
        "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255,0,0), 5) # box\n",
        "        cv2.circle(frame, (center_x,center_y), 5,(255,0,0),-1) # center of box\n",
        "\n",
        "        #Drawing above the bounding-box the name of class recognized.\n",
        "        cv2.putText(img=frame, text=labels[ix]+' - '+str(np.round(conf[ix],2)),\n",
        "                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 0, 0),thickness=2)\n",
        "\n",
        "        # Checking if the center of recognized vehicle is in the area given by the transition line + offset and transition line - offset\n",
        "        if (center_y < (cy_linha + offset)) and (center_y > (cy_linha - offset)):\n",
        "            if  (center_x >= 0) and (center_x <=cx_sentido):\n",
        "                contador_in +=1\n",
        "                veiculos_contador_in[category] += 1  #number of vechicals coming in\n",
        "            else:\n",
        "                contador_out += 1 #number of vechicals coming out\n",
        "                veiculos_contador_out[category] += 1\n",
        "\n",
        "    #updating the counting type of vehicle\n",
        "    contador_in_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_in.items()]\n",
        "    contador_out_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_out.items()]\n",
        "\n",
        "    #drawing the number of vehicles in\\out\n",
        "    cv2.putText(img=frame, text='N. vehicles In',\n",
        "                org= (30,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
        "                fontScale=1, color=(255, 255, 0),thickness=1)\n",
        "\n",
        "    cv2.putText(img=frame, text='N. vehicles Out',\n",
        "                org= (int(2800 * scale_percent/100 ),30),\n",
        "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=1)\n",
        "\n",
        "    #drawing the counting of type of vehicles in the corners of frame\n",
        "    xt = 40\n",
        "    for txt in range(len(contador_in_plt)):\n",
        "        xt +=30\n",
        "        cv2.putText(img=frame, text=contador_in_plt[txt],\n",
        "                    org= (30,xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
        "                    fontScale=1, color=(255, 255, 0),thickness=1)\n",
        "\n",
        "        cv2.putText(img=frame, text=contador_out_plt[txt],\n",
        "                    org= (int(2800 * scale_percent/100 ),xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n",
        "                    fontScale=1, color=(255, 255, 0),thickness=1)\n",
        "\n",
        "    #drawing the number of vehicles in\\out\n",
        "    cv2.putText(img=frame, text=f'In:{contador_in}',\n",
        "                org= (int(1820 * scale_percent/100 ),cy_linha+60),\n",
        "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
        "\n",
        "    cv2.putText(img=frame, text=f'Out:{contador_out}',\n",
        "                org= (int(1800 * scale_percent/100 ),cy_linha-40),\n",
        "                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n",
        "\n",
        "    if verbose:\n",
        "        print(contador_in, contador_out)\n",
        "    #Saving frames in a list\n",
        "    frames_list.append(frame)\n",
        "    #saving transformed frames in a output video formaat\n",
        "    output_video.write(frame)\n",
        "\n",
        "#Releasing the video\n",
        "output_video.release()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QtjpUW8DETyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Processing"
      ],
      "metadata": {
        "id": "evBQKiHTC9L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####  pos processing\n",
        "# Fixing video output codec to run in the notebook\\browser\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "subprocess.run(\n",
        "    [\"ffmpeg\",  \"-i\", tmp_output_path,\"-crf\",\"18\",\"-preset\",\"veryfast\",\"-hide_banner\",\"-loglevel\",\"error\",\"-vcodec\",\"libx264\",output_path])\n",
        "os.remove(tmp_output_path)"
      ],
      "metadata": {
        "id": "RCQMeWEJsHji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling Transformed Frame Results"
      ],
      "metadata": {
        "id": "0jN6izYTDA_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking samples of processed frames\n",
        "for i in [28, 29, 32, 40, 42, 50]:\n",
        "    plt.figure(figsize =( 14, 10))\n",
        "    plt.imshow(frame_list[i])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rT6i11MYEg0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing Result Video"
      ],
      "metadata": {
        "id": "x6YuyujzDMa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#output video result\n",
        "frac = 0.7\n",
        "Video(data='rep_result.mp4', embed=True, height=int(720 * frac), width=int(1280 * frac))"
      ],
      "metadata": {
        "id": "TdCcocp7MOad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "PKYeW4jEHptV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Our  model is working fine, and is making **accurate** predictions.\n",
        "\n",
        "- We can see **bounding box** around each object with the **class name** and the **class probability** at the **top left** corner of the box.\n"
      ],
      "metadata": {
        "id": "Nir4Lnl2Hw-X"
      }
    }
  ]
}